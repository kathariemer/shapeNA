---
title: "M-estimation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{M-estimation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(shapeNA)
library(mvtnorm)
```

## Power M-estimator

```{r}
set.seed(1)
mu <- c(0, 0)
S <- matrix(c(1, 0.5, 0.5, 2), ncol = 2)
X <- rmvt(500, sigma = S, df = 8)
```

Here, we have data from a multivariate `t`-distribution with 5 degrees of freedom. We know, that

$$\mu = (0, 0)\text{ and } \Sigma = \begin{pmatrix} 1 & 0.5 \\ 0.5 & 2\end{pmatrix}.$$

When using the classic estimators, we get:

```{r}
muHat <- colMeans(X)
round(muHat, 2)
SigmaHat <- cov(X)
round(SigmaHat, 2)
```

Clearly, the computed scatter matrix differs quite a bit from the true covariance matrix. This is due to the fact, that the sample covariance matrix is heavily influenced by outliers.

We may plot both the true parameters and the estimated parameters. The red ellipse represents the points in the plane, which for the true location and scatter, have mahalanobis distance 1. We can see, that the classical estimator, the represented by the dashed black line, clearly overestimates the size of the covariance matrix.

```{r, echo=FALSE}
par(mar = c(4,4,0,0)+0.1)
plot(X, col = 8, cex = 0.5, xlab='X1', ylab='X2')
lines(shapeNA:::ellipse(mu, S), col = 2, lwd = 2)
points(t(mu), col = 2, pch = 3, lwd = 2)
lines(shapeNA:::ellipse(muHat, SigmaHat), col = 1, lty = 3, lwd = 2)
points(t(muHat), col = 1, pch = 4, lwd = 2)
```

The power M-estimators give, with increasing `alpha`, closer estimates. The parameter `alpha` is also considered to be a tail index, and should be chosen according to the heavyness of the data's tails. In the following plot, 

* the red line represents the classical scatter estimate (`alpha = 0`), 
* the dashed green line comes from an estimate with `alpha = 1/3` and
* the dotted blue line comes from an estimate with `alpha = 2/3`.

```{r, echo = FALSE}
par(mar = c(4,4,0,0)+0.1)
plot(X, xlim = c(-0.1, 2), ylim = c(-0.1, 2), col = 8, xlab='X1', ylab='X2')
for (i in 1:3) {
  res <- powerShape(X, alpha = (i-1)/3)
  lines(shapeNA:::ellipseShape(res), col = i+1, lwd = 2.5, lty = i)
  points(t(res$mu), col = i+1, pch = i+1)
}
```

In the special case of `alpha = 0`, we get the traditional covariance and location estimate:

```{r}
res <- powerShape(X, alpha = 0)
summary(res)
```


```{r}
all.equal(res$mu, muHat)
all.equal(shape2scatter(res), SigmaHat)
```

In the special case of `alpha == 1`, for Tyler's M-estimate, we only get shape estimates, instead of covariance estimates. That means, the scale of the resulting matrix is undefined. Per default, they are computed to have determinant 1, even though other normalizations may be chosen.
